import requests
import concurrent.futures
import argparse
import urllib3
from urllib3.exceptions import InsecureRequestWarning

# Disable InsecureRequestWarning
urllib3.disable_warnings(InsecureRequestWarning)

# Define sensitive file paths to check
sensitive_paths = [
    "/.svn/wc.db",
    "/.hg/store/00manifest.i",
    "/.env",
    "/.well-known/security.txt",
    "/security.txt"
]

# Define common Git directory files and paths to check
git_well_known_paths = [
    "HEAD",
    "ORIG_HEAD",
    "description",
    "config",
    "COMMIT_EDITMSG",
    "index",
    "packed-refs",
    "objects/info/packs",
    "refs/heads/master",
    "refs/heads/main",
    "refs/remotes/origin/HEAD",
    "refs/stash",
    "logs/HEAD",
    "logs/refs/stash",
    "logs/refs/heads/master",
    "logs/refs/heads/main",
    "logs/refs/remotes/origin/HEAD",
    "info/refs",
    "info/exclude"
]

# Check if the website exposes sensitive files
def check_path(url, path, timeout):
    try:
        response = requests.get(url + path, timeout=timeout, verify=False)
        if response.status_code == 200:
            return url + path
    except requests.Timeout:
        pass  # Ignore timeout errors
    except requests.RequestException as e:
        if isinstance(e, requests.exceptions.SSLError):
            print(f"[SSL Error] {url}{path}: {e}")
        elif not isinstance(e, requests.exceptions.ReadTimeout):
            print(f"[Error] {url}{path}: {e}")
    return None

def check_website(url, max_workers, timeout):
    paths_to_check = sensitive_paths + [f"/.git/{path}" for path in git_well_known_paths]
    risks = []

    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(check_path, url, path, timeout) for path in paths_to_check]
        for future in concurrent.futures.as_completed(futures):
            result = future.result()
            if result:
                risks.append(result)

    return risks

def is_valid_url(url):
    # Simple check to see if the URL starts with http:// or https://
    return url.startswith('http://') or url.startswith('https://')

def main():
    # Set up command-line argument parsing
    parser = argparse.ArgumentParser(description='Check if websites expose sensitive files')
    parser.add_argument('-t', '--threads', type=int, default=10, help='Specify number of threads (default is 10)')
    parser.add_argument('-o', '--timeout', type=int, default=1, help='Specify timeout in seconds (default is 1 second)')
    args = parser.parse_args()

    # Read the list of websites and filter out invalid URLs
    with open('weblist.txt', 'r') as file:
        websites = [line.strip() for line in file.readlines() if is_valid_url(line.strip())]

    print("Starting scan...")

    # Output risky URLs
    risky_urls = []
    leaky_sites = []
    total_sites_with_leaks = 0

    with concurrent.futures.ThreadPoolExecutor(max_workers=args.threads) as executor:
        futures = {executor.submit(check_website, website, args.threads, args.timeout): website for website in websites}
        for future in concurrent.futures.as_completed(futures):
            website = futures[future]
            try:
                risks = future.result()
                if risks:
                    total_sites_with_leaks += 1
                    leaky_sites.append(website)
                    risky_urls.extend(risks)
                    print(f"[Found] {len(risky_urls)} Page(s) in {total_sites_with_leaks} Site(s)")
            except Exception as e:
                print(f"[Error] {website}: {e}")

    # Write risky URLs to a file
    with open('risky_urls.out', 'w') as output_file:
        for url in risky_urls:
            output_file.write(url + '\n')

    if leaky_sites:
        print(f"[Found] {len(risky_urls)} Page(s) in {total_sites_with_leaks} Site(s)")
        print("Sites with leaks:")
        for site in leaky_sites:
            print(site)
    else:
        print("No sites with leaks found.")

    print("Scan completed.")

if __name__ == "__main__":
    main()
