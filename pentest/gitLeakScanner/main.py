import requests
import concurrent.futures
import argparse
import urllib3
from urllib3.exceptions import InsecureRequestWarning
from tqdm import tqdm
import sublist3r

# Disable InsecureRequestWarning
urllib3.disable_warnings(InsecureRequestWarning)

# Define common Git directory files and paths to check
git_paths = [
    "/.git/HEAD",
    "/.git/config"
]

# Check if the website exposes .git files
def check_git_path(url, path, timeout):
    try:
        response = requests.get(url + path, timeout=timeout, verify=False)
        if response.status_code == 200:
            content = response.text
            if path.endswith("HEAD") and "ref: refs/heads/" in content:
                return url + path
            elif path.endswith("config") and "[core]" in content:
                return url + path
    except requests.Timeout:
        pass  # Ignore timeout errors
    except requests.RequestException as e:
        if isinstance(e, requests.exceptions.SSLError):
            print(f"[SSL Error] {url}{path}")
        elif isinstance(e, requests.exceptions.ConnectionError) and 'getaddrinfo failed' in str(e):
            pass
        elif not isinstance(e, requests.exceptions.ReadTimeout):
            #print(f"[Error] {url}{path} :{e}")
            pass
    return None

def check_website(url, max_workers, timeout):
    risks = []

    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(check_git_path, url, path, timeout) for path in git_paths]
        for future in concurrent.futures.as_completed(futures):
            result = future.result()
            if result:
                risks.append(result)

    return risks

def is_valid_url(url):
    # Simple check to see if the URL starts with http:// or https://
    return url.startswith('http://') or url.startswith('https://')

def get_subdomains(domain):
    try:
        subdomains = sublist3r.main(domain, 40, None, ports=None, silent=True, verbose=False, enable_bruteforce=False, engines=None)
        return subdomains
    except NameError as e:
        if "name 'unicode' is not defined" in str(e):
            print(f"[Error] unicode error encountered for {domain}")
        else:
            print(f"[Error] Failed to get subdomains for {domain}: {e}")
        return []

def main():
    # Set up command-line argument parsing
    parser = argparse.ArgumentParser(description='Check if websites expose .git files')
    parser.add_argument('-t', '--threads', type=int, default=10, help='Specify number of threads (default is 10)')
    parser.add_argument('-o', '--timeout', type=int, default=1, help='Specify timeout in seconds (default is 1 second)')
    parser.add_argument('-d', '--subdomains', action='store_true', help='Include subdomain scanning')
    args = parser.parse_args()

    # Read the list of websites and filter out invalid URLs
    with open('weblist.txt', 'r') as file:
        websites = [line.strip() for line in file.readlines() if is_valid_url(line.strip())]

    if args.subdomains:
        print("Starting subdomain scan...")

        # Collect all subdomains
        all_subdomains = set()
        with concurrent.futures.ThreadPoolExecutor(max_workers=args.threads) as executor:
            future_to_domain = {executor.submit(get_subdomains, website.split('//')[-1].split('/')[0]): website for website in websites}
            for future in tqdm(concurrent.futures.as_completed(future_to_domain), total=len(future_to_domain), desc="Finding subdomains", unit="domain"):
                domain = future_to_domain[future]
                try:
                    subdomains = future.result()
                    all_subdomains.update(subdomains)
                except Exception as e:
                    print(f"[Error] Failed to get subdomains for {domain}: {e}")
        print("New subdomains found:")
        print(all_subdomains)

        print("Subdomain scan completed.")
        websites.extend([f"http://{sub}" for sub in all_subdomains])

    # Output risky URLs
    risky_urls = []
    leaky_sites = []
    total_sites_with_leaks = 0

    print("Starting .git scan...")

    # Scan for .git exposure in all (sub)domains
    with concurrent.futures.ThreadPoolExecutor(max_workers=args.threads) as executor:
        futures = {executor.submit(check_website, website, args.threads, args.timeout): website for website in websites}
        
        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc="Scanning websites", unit="website"):
            website = futures[future]
            try:
                risks = future.result()
                if risks:
                    total_sites_with_leaks += 1
                    leaky_sites.append(website)
                    risky_urls.extend(risks)
            except Exception:
                print(f"[Error] {website}")

    # Write risky URLs to a file
    with open('risky_urls.out', 'w') as output_file:
        for url in risky_urls:
            output_file.write(url + '\n')

    if leaky_sites:
        print(f"[Found] {len(risky_urls)} Page(s) in {total_sites_with_leaks} Site(s)")
        print("Sites with leaks:")
        for site in leaky_sites:
            print(site)
    else:
        print("No sites with leaks found.")

    print("Scan completed.")

if __name__ == "__main__":
    main()
